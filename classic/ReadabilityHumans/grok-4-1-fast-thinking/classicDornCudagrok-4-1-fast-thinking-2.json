{"score":"15","reasoning":"The code is a dense macro expansion with excessively long lines listing many variables (b0-b15, a-e, p0-p15) in function calls like SHA_TRANSFORM_SMALL and checkHashMultiSHA1. Heavy macro usage (e.g. incrementCounters##length##Multi) obscures logic and intent. Lacks comments, whitespace, or structure, making it extremely difficult to read and comprehend without full context.","tokens":970,"name":"98.jsnp"}
{"score":"65","reasoning":"Code follows logical flow for GPU convolution per slice: padding kernel and data, forward FFTs, modulation, inverse FFT, cropping. Structure is clear with good indentation. However, cluttered by numerous commented debug prints, inline error checks, FIXME note, and reliance on non-standard macros like cutilSafeCall. Variables like fftH are assumed defined elsewhere. Readable for CUDA experts but lacks cleanliness for general comprehension.","tokens":1558,"name":"20.jsnp"}
{"score":"95","reasoning":"This is a clean Python list fragment of strings, likely directory paths. Each item is on its own line with consistent indentation, making it highly readable and easy to scan. Content is straightforward and self-explanatory. Slight deduction as it is incomplete without opening bracket or assignment.","tokens":886,"name":"96.jsnp"}
{"score":"45","reasoning":"Code snippet is fragmented, starting mid-kernel with abrupt if statement and extra braces. Heavy repetition in switch cases for different block sizes lacks abstraction like a loop or macro, making it verbose and hard to maintain. Custom types like cudafloat and macros like CUDA_SQRT assume prior knowledge, reducing general readability. Intent is clear for CUDA experts but overall comprehension is moderate due to redundancy and incompleteness.","tokens":1495,"name":"36.jsnp"}
{"score":"92","reasoning":"Short, linear function with clear sequential steps: prepare data and raycaster with early returns on failure, generate image, clear resources, and return success. Descriptive function names aid comprehension. No unnecessary complexity or nesting. Minor deduction for lack of comments and domain-specific assumptions.","tokens":973,"name":"57.jsnp"}
{"score":"72","reasoning":"Code demonstrates standard CUDA kernel launch with proper grid and block dimension calculation using ceil for grid size. Readable for CUDA experts with clear dim3 usage and synchronization. Issues include commented-out line, likely typo in d_jont_hist, unclear et_ kernel prefix, and reliance on undefined variables like BLOCK, array_size, reducing overall comprehension for general readers.","tokens":1066,"name":"103.jsnp"}
{"score":"88","reasoning":"Code implements efficient warp-unrolled min reduction in CUDA shared memory, standard idiom for performance. Repetitive if-statements for powers-of-2 strides are clear to experts but verbose. Volatile pointer and EMUSYNC macro assume CUDA knowledge. Minor indentation issues, no comments, but logic is straightforward and idiomatic.","tokens":1105,"name":"21.jsnp"}
{"score":"72","reasoning":"CUDA kernel snippet for DE algorithm shows good structure with mutation crossover and selection phases marked by comments and switches. Uses standard CUDA idioms like threadIdx blockIdx syncthreads and shared vars. However dense nested conditions magic vars like f jr L overflow and macros IMUL BETTER_THAN reduce immediate readability. Assumes DE domain knowledge abrupt snippet ends lower score.","tokens":1265,"name":"46.jsnp"}
{"score":"68","reasoning":"Code structure is logical with descriptive variables like jointEntropyDerivative_X and clear conditional checks for NaN and bounds. Nested loops approximate Parzen window effectively. However, comments have typos and errors like resample instead of resampled, incomplete sentences. Magic numbers like c_Binning and abrupt end reduce full comprehension. Readable for CUDA experts but less for novices.","tokens":1418,"name":"81.jsnp"}
{"score":"82","reasoning":"Code is clean and uses descriptive variable names like resultImageGradientTexture and logJointHistogram_d, making intent clear for CUDA experts. CUDA_SAFE_CALL macro aids safety. Grid and block setup is standard with proper ceil calculation. Minor issues: lacks comments, magic number 0 in bindTexture calls, assumes predefined constants like Block_reg_getVoxelBasedNMIGradientUsingPW. Highly readable for domain specialists, moderately for others.","tokens":1004,"name":"67.jsnp"}
{"score":"35","reasoning":"Code is dense and hard to follow due to cryptic variable names like bchan echan po.sp1, magic numbers such as n_bin/2/po.nchan and hardcoded 80 for msg. Assumes deep knowledge of PGPLOT library and plot_option struct. Static variables introduce hidden state. Nested loops for min/max lack clarity. Incomplete at end with cutoff loop. Minimal comments and some commented-out code reduce readability. Indentation is okay but overall comprehension is low without domain context.","tokens":1441,"name":"112.jsnp"}
{"score":"72","reasoning":"Code uses clear CUDA host function naming with h_ prefix and standard kernel launches. Short functions aid comprehension. However, inconsistent spacing in parameters, long arg lists, missing inline comments, and assumptions like iAlignUp reduce readability for non-experts. Error checks are consistent but messages slightly vary.","tokens":1716,"name":"47.jsnp"}
{"score":"68","reasoning":"Code snippet is fragmented, starting mid-function with missing prior declarations for inArgs, outArgs, sem_in, sem_out, reducing comprehension without full context. Helpful explanatory comment despite typos like garuanteed. Logic is simple semaphore-based threading but relies on globals. Indentation inconsistent, overall moderately readable for experienced C/CUDA devs.","tokens":1299,"name":"60.jsnp"}
{"score":"20","reasoning":"This snippet is the tail end of a function call listing parameters, likely CUDA device variables based on naming like seqs_mapQ_de. Names follow a consistent pattern but are cryptic without types, context, or full function. No comments or structure provided, rendering it hard to comprehend purpose or logic. Minimal readability due to isolation.","tokens":874,"name":"35.jsnp"}
{"score":"65","reasoning":"Code has clear structure for GPU 2D separable convolution with logical slice loop and const dimensions aiding readability. However, complex pointer-to-pointer params, magic numbers like 2*kernel_size[0], unused status var, abrupt incomplete end without braces or cudaFree, FIXME note, and undefined external functions hinder full comprehension for non-experts.","tokens":1159,"name":"68.jsnp"}
{"score":"25","reasoning":"Code snippet is incomplete, ending abruptly after computing 2D thread indices. Uses non-standard KERNEL instead of __global__ for CUDA kernel. Parameter declarations lack spaces after commas, making it hard to parse. No comments or further logic, limiting comprehension. Basic structure recognizable but poor formatting reduces readability.","tokens":1052,"name":"58.jsnp"}
{"score":"88","reasoning":"Code is concise with short methods each handling one task clearly: lazy generator init, cleanup, seeding, and filling arrays. Names like RandomGenerator and CleanUp are intuitive. Uses modern C++ like nullptr. Assumes cuRAND familiarity but logic flows well. Minor nit: atexit may register multiple times but idempotent. High readability and comprehension.","tokens":1295,"name":"116.jsnp"}
{"score":"65","reasoning":"Inconsistent indentation hinders structure comprehension. C-style memory management with manual frees in C++ destructor lacks modern safety. Variable names are descriptive, logic in init, destructor, and run method is straightforward. Printf outputs are clear. Missing error checks and abrupt snippet end slightly reduce ease of understanding.","tokens":1304,"name":"73.jsnp"}
{"score":"62","reasoning":"Incomplete snippet starts abruptly mid-kernel after a loop, missing full context which hinders comprehension. Indentation inconsistent. Cryptic variable names like idnx idny Csub. Relies on undefined BLOCK_SIZE. Standard CUDA tiled structure and host launcher are readable. Minor typos like Euclidian. Overall moderate readability for CUDA experts.","tokens":1371,"name":"101.jsnp"}
{"score":"52","reasoning":"The code has a logical flow: index calculation, bounds check, register management, loop with MD5 computation and charset reduction, conditional output/save. However, readability is hampered by excessive line continuations with backslashes, very long function calls listing 16 b-registers individually instead of arrays, long verbose variable names, and lack of comments or whitespace. Comprehensible to CUDA experts but tedious for others.","tokens":1495,"name":"93.jsnp"}
{"score":"68","reasoning":"Code structure is linear and easy to follow, with descriptive variable names like hostPtr and devPtr. Initialization loop and error checks on malloc and cudaMalloc aid comprehension. However, missing stdlib.h for malloc/free, no free(backPtr) or cudaFree(devPtr), absent error checks on cudaMemcpy, unexplained commented mlock/munlock sections, magic number 126, and no transfer verification or success return lower readability.","tokens":1477,"name":"31.jsnp"}
{"score":"72","reasoning":"Code flow is linear and easy to follow: checks pointers, initializes host memory, allocates device memory, performs HtoD and DtoH copies. Error handling on allocations is good. Issues: no error checks on cudaMemcpy calls, missing cudaFree for devPtr leak, undeclared i and TRANSFER_SIZE in snippet, inconsistent const casts, free only hostPtr. Commented mlock clear. Readable but sloppy and incomplete.","tokens":1368,"name":"39.jsnp"}
{"score":"78","reasoning":"Code snippet sets up CUDA symbols, textures, and kernel grid for NMI gradient computation. Descriptive names like activeVoxelNumber and section comments aid readability. Standard CUDA idioms with macros like CUDA_SAFE_CALL are used effectively. Minor issues: long lines, external dependencies like Block_reg_getVoxelBasedNMIGradientUsingPW, and abrupt end reduce full comprehension slightly.","tokens":1479,"name":"89.jsnp"}
{"score":"35","reasoning":"The code is highly repetitive, duplicating nearly identical accumulation patterns for displacement and Tx Ty Tz components across multiple basis combinations without loops or abstraction, leading to verbosity and maintenance issues. Cryptic variable names like xBasis tempBasis nodeCoefficient lack clarity. No comments explain the spline evaluation logic. Consistent but dense structure aids experts slightly yet burdens general comprehension.","tokens":1561,"name":"50.jsnp"}
{"score":"62","reasoning":"This CUDA snippet computes a neuron output and gradients using shared memory reduction. It is efficient and idiomatic for experts, with proper sync and warp handling. However, heavy reliance on undefined macros (NEURON, PATTERN, CUDA_SIGMOID, BIAS), external functions (SumBeforeWarp, SumWarp), and variables (mOffset, m) obscures intent. Dense logic without comments lowers comprehension for non-experts.","tokens":1428,"name":"61.jsnp"}
{"score":"82","reasoning":"Code is concise with descriptive variable names like invR tbot ttop and uses standard CUDA functions effectively for ray-box intersection via slab method and matrix transforms. Includes helpful link in comment. Minor issues: apparent bugs in fmaxf fminf aggregations (repeats tmin.x instead of tmin.z) hinder full comprehension and snippet starts with incomplete struct fragment reducing context clarity.","tokens":1717,"name":"90.jsnp"}
{"score":"82","reasoning":"Code uses standard CUDA intrinsics and vector types effectively for ray-slab intersection and matrix multiplication. Functions are concise and purposeful. Redundant but correct max/min reductions on float3 components slightly hinder clarity. Kernel initialization with descriptive constants aids readability. Assumes CUDA familiarity, suitable for experts but dense for novices.","tokens":1866,"name":"28.jsnp"}
{"score":"87","reasoning":"Standard CUDA parallel reduction for min value and position in shared memory, using tree-based pairwise comparisons with syncthreads. Repetitive unrolled loops are performant but slightly verbose. Volatile pointers correctly handle warp-level reduction. Clear for experts, assumes CUDA knowledge, snippet lacks full context but logic is straightforward.","tokens":1479,"name":"99.jsnp"}
{"score":"28","reasoning":"The snippet is a dense CUDA kernel fragment with extremely long function calls spanning multiple lines via backslashes, overloaded with 19+ parameters like p0-p15 and b0-b15. Macro expansions like ##length##Multi add obscurity. No comments, poor visual structure, and heavy reliance on context make it hard to comprehend quickly despite domain familiarity.","tokens":937,"name":"92.jsnp"}
{"score":"72","reasoning":"Code is structured with clear steps for texture binding memory allocation kernel launch and cleanup using consistent CUDA_SAFE_CALL macro and comments. However readability suffers from undeclared external variables like activeVoxelNumber and arrays magic numbers redundant comments dense matrix handling with float4 and assumption of prior context making it harder to comprehend standalone.","tokens":1426,"name":"23.jsnp"}
{"score":"42","reasoning":"Code structure is functional with defines and inline utility but lacks comments explaining matrix operations or layout. Hardcoded array indices for matrix elements reduce clarity. Manual calloc and free calls are error-prone without error checks. Unused iDivUp function adds confusion. Assumes external types like mat_44 and functions without context. Readable for linear algebra experts but challenging otherwise.","tokens":1772,"name":"100.jsnp"}
{"score":"92","reasoning":"Code is highly readable with clear, labeled printf statements using tabs for alignment. Comments succinctly describe actions like loading matrix and database. Simple function calls with intuitive names. Minor deduction for C++ style comments in apparent C code and undeclared variables in snippet, but overall very easy to comprehend.","tokens":941,"name":"107.jsnp"}
{"score":"88","reasoning":"Code is clean and professional C socket programming with good error handling using perror and exit. Variable names are descriptive like sock_fd client_fd caddr. Logic in net_accept and net_recv is clear and follows standard patterns for IPv6. Minor deductions for incomplete snippet abrupt start no comments and unusual close of listening socket on accept error. Easy to comprehend for C experts.","tokens":1247,"name":"102.jsnp"}
{"score":"88","reasoning":"Clear and concise C code for accumulating bytes, verbose progress print, and loop exit condition. Logic is straightforward with good variable names. Float cast is verbose but understandable. Comment aids comprehension. Minor deduction for snippet incompleteness starting with closing brace.","tokens":1062,"name":"65.jsnp"}
{"score":"85","reasoning":"Well-structured CUDA kernels with clear purpose via comments. Efficient shared memory and template-based reduction for performance. Good use of IMUL for indexing and conditional compilation. Assumes CUDA expertise; dense indexing and magic numbers slightly reduce readability. Incomplete kernel closure impacts comprehension.","tokens":1450,"name":"42.jsnp"}
{"score":"25","reasoning":"The code uses multiple line continuation backslashes which obscure readability. Empty if block followed by else is awkward and could be simplified by inverting the condition. The function call has an excessively long unformatted argument list spanning many registers b0-b15 plus others. Indentation is inconsistent and sparse context makes comprehension difficult.","tokens":1057,"name":"55.jsnp"}
{"score":"35","reasoning":"Code implements trilinear interpolation for volume sampling with 8 corner points and weights, but suffers from cryptic variable names like p101 p001, no comments, high repetition in weight calculations and array indexing, long unwrapped lines, and assumes undefined prior variables, reducing readability and comprehension for most readers.","tokens":1323,"name":"82.jsnp"}
{"score":"35","reasoning":"Heavy reliance on macros like SH and SVW obscures array indexing and transposition. Magic numbers 32 and 16 dominate without definition. Complex conditional assignments in tiled loops with tx offset are confusing. Undefined macros like HMATRIX and CUDA_VALUE hinder understanding. No comments or output writes. Optimized CUDA but low readability for most.","tokens":1587,"name":"7.jsnp"}
{"score":"95","reasoning":"The code snippet defines a standard MAX macro using a ternary operator, properly parenthesized for safety. It includes CUDA-specific headers like cutil_inline.h, vector_types.h, and others, with one commented out include. Structure is simple, no complex logic, excellent readability and easy comprehension for C/CUDA developers.","tokens":878,"name":"88.jsnp"}
{"score":"72","reasoning":"Code shows good structure with comments explaining sections and logical flow from log setup to test execution. Variable names are mostly descriptive, but abbreviations like htod, dtoh, dtod, wc reduce clarity for non-experts. Incomplete snippet cuts off mid-function, hindering full understanding. Suitable for experienced C/CUDA developers.","tokens":1061,"name":"5.jsnp"}
{"score":"72","reasoning":"The snippet performs a straightforward backprojection computation in CUDA, accumulating attenuation and applying exponential decay. Uses familiar CUDA elements like threadIdx.x. Code is concise and logical, but suffers from poor indentation, missing context for loops and variables, and a distracting commented line. Readable for CUDA experts, less so for novices.","tokens":1022,"name":"12.jsnp"}
{"score":"35","reasoning":"Code is dense CUDA kernel snippet for neural net gradient computation with parallel reduction. Lacks comments, uses cryptic names like lgNextLayer, localGradientNextLayer, and magic constants like NUM_OUTPUTS. Complex indexing and bit-shift loop are efficient but obscure intent. Incomplete at end. Readable by CUDA experts only, low general comprehension.","tokens":1219,"name":"18.jsnp"}
{"score":"68","reasoning":"Code handles endian conversion via bit shifts, binary search adjustment, goto for perf, and backward linear scan for hash matches. Readability impacted by goto usage, magic numbers like 0xff, dense logic, and undefined globals like DEVICE_Hashes_32. Comments provide some clarity, but variable names are okay yet cryptic. Comprehensible to experts with Cuda context, but not beginner-friendly.","tokens":1186,"name":"19.jsnp"}
{"score":"75","reasoning":"CUDA kernel snippet for ray backprojection shows good structure with const locals clear names and early bounds check. Standard idioms like threadIdx blockIdx and float3 usage aid readability for experts. Minor issues: incomplete code undefined MAX_STEPS and matrix mul context reduce broad comprehension.","tokens":1296,"name":"14.jsnp"}
{"score":"82","reasoning":"Well-structured CUDA function with clear sequential steps: compute sizes, copy to symbols, bind textures, memset output, launch kernel. Descriptive variable names like voxelNumber, binNumber. Uses safe macros and debug print. Drawbacks: very long function and param names, external dependencies like Block_reg_getVoxelBasedNMIGradientUsingPW and kernels reduce standalone readability, assumes CUDA expertise.","tokens":1673,"name":"64.jsnp"}
{"score":"82","reasoning":"Well-structured CUDA kernels implementing simulated annealing steps with proper thread block indexing and synchronization via shared memory and syncthreads. Clear logic for position updates and selection based on fitness and temperature. Deductions for undefined macros like IMUL BETTER_THAN and cropPosition function inline preprocessor directives and assumed 2D grid layout which may confuse non-experts but fine for CUDA knowledgeable readers.","tokens":1591,"name":"0.jsnp"}
{"score":"72","reasoning":"CUDA kernel finds argmin index per row clearly using loop, standard indexing. Host setup uses fixed block size, computes grid dims logically. Pros: concise logic, proper includes, full license. Cons: unused vars bx and idnx, no inline comments, incomplete host function, assumes cudafloat defined. Readable for experts, minor confusions reduce ease.","tokens":1791,"name":"106.jsnp"}
{"score":"72","reasoning":"Code follows standard CUDA kernel structure with proper thread indexing and bounds checking. Logic for initializing biases deltas and learning rates is clear and concise. Custom macros like KERNEL cudafloat and conditional compilation add some opacity requiring header context. Lack of comments and incomplete second kernel slightly hinder full comprehension.","tokens":1301,"name":"3.jsnp"}
{"score":"35","reasoning":"Snippet is a CUDA kernel launch fragment lacking context: undefined variables G1 B1 d_activity d_attenuation currentCamPointer img. Commented out malloc adds noise without purpose. No explanations for macros or kernel function. Correct syntax but poor readability and comprehension due to incompleteness and assumptions of prior knowledge.","tokens":1047,"name":"72.jsnp"}
{"score":"88","reasoning":"Code is concise and follows standard ray-box intersection slab method. Clear variable names like invR tbot ttop tmin tmax enhance readability. Uses familiar CUDA intrinsics like make_float3 fminf fmaxf. Includes source reference comment. Minor redundancy in max min calls but functionally correct. Incomplete snippet slightly impacts full comprehension.","tokens":1366,"name":"13.jsnp"}
{"score":"35","reasoning":"The snippet is very short and consists of simple CUDA synchronization and increment operations, which are readable due to straightforward syntax. However, it lacks context, variable declarations, loop conditions, and overall structure, making it difficult to comprehend its purpose or flow as a standalone piece. The abrupt closing braces add to the confusion.","tokens":816,"name":"62.jsnp"}
{"score":"65","reasoning":"The code performs simple pointer assignments from a matrix using query indices in what appears to be nested loops, then CUDA memory copy and free. It is straightforward for those familiar with C and CUDA, with a clear pattern of loading four values per iteration. However, lacks context like variable declarations, types, and loop conditions. Cryptic names like p, matrix, query. Possible typos in pMemcpy2DToArray and pFreeHost. Minimal helpful comment. Moderately readable but assumes prior knowledge.","tokens":1221,"name":"85.jsnp"}
{"score":"65","reasoning":"The code includes a detailed GPL license header and standard CUDA includes. Thread and block indexing uses common patterns with clear variable names like idnx and idny. Indentation is consistent. However, the kernel body is absent, making it incomplete and limiting full comprehension of functionality. No inline comments explain the purpose.","tokens":1177,"name":"111.jsnp"}
{"score":"25","reasoning":"Confusing macros with inconsistent row-column swapping and commented alternatives hinder understanding. Non-standard KERNEL macro and invalid CUDA shared memory syntax like cudafloat SH(32,32) suggest errors. Undeclared sh and svw variables referenced in macros add confusion. Incomplete code lacks context, severely impacting readability and comprehension.","tokens":1305,"name":"11.jsnp"}
{"score":"70","reasoning":"Code is short and structured with basic comments and standard CUDA idioms like tid calculation and bounds check. Variable names are descriptive with g_ prefix. However, shared memory load lacks syncthreads and appears unused for sharing since each thread reads its own write. BLOCK constant undefined here. Loop logic clear but assumes domain knowledge of backprojection. Minor issues like unnecessary return reduce perfect score.","tokens":1587,"name":"104.jsnp"}
{"score":"88","reasoning":"Simple concise function using cudaMemset to zero GPU accumulator memory based on nifti_image dimensions. Highly readable for CUDA experts. Deductions for unused BLOCK define which adds confusion and the long single-line memset call that could benefit from formatting.","tokens":936,"name":"105.jsnp"}
{"score":"95","reasoning":"Code is clean, with descriptive function names, clear comments, and straightforward logic for file opening and error handling. Uses standard C practices like checking fopen return and exiting on failure. Minor duplication between open_input and open_output prevents perfect score, but overall very readable and comprehensible for any C programmer.","tokens":1068,"name":"41.jsnp"}
{"score":"25","reasoning":"Dense CUDA kernel using macros like ##length for generation, unrolled password bytes as p0-p15 and blocks b0-b15 with single-letter vars a b c d. Long argument lists, shared memory, external functions like initMD CUDA_MD4. No comments, relies on context for charset bitmap handling. Poor readability for general audience despite clear loop structure.","tokens":1588,"name":"24.jsnp"}
{"score":"45","reasoning":"Code is compact with consistent register prefixes but uses highly cryptic variable names like regT regE0.y regF which hinder comprehension without context. Operations like sub_sat and max are terse. Comments provide some insight but reference undefined vecH vecE vecShift terms adding confusion. Suitable only for experts familiar with CUDA alignment algorithms.","tokens":975,"name":"97.jsnp"}
{"score":"82","reasoning":"The code snippet presents a CUDA kernel reduction tail using syncthreads, SumBeforeWarp, and warp-level SumWarp, followed by a switch-based launcher for templated SumSmallArray kernels across block sizes. Readability is good for CUDA experts due to standard reduction patterns and clear intent, but repetitive switch cases are verbose and lack a default, slightly hindering conciseness and robustness. Overall comprehensible with minor improvements possible.","tokens":1749,"name":"51.jsnp"}
{"score":"78","reasoning":"Code is well-structured for CUDA experts with clear steps: constant setup symbol and texture binding memset kernel launch. Uses safe macros and standard patterns. However long functionparam names cramped lines external dependencies like Block size and kernel reduce readability for non-experts. Assumes deep CUDA knowledge.","tokens":1535,"name":"34.jsnp"}
{"score":"68","reasoning":"Code uses helpful comments and CUDA_SAFE_CALL macros for safety. However, inconsistent indentation, cramped lines like variable declaration followed by cudaMalloc without space, missing spaces around operators, magic numbers like 65335 and 65535 unexplained, and abrupt formatting make it harder to read. Suitable for experienced CUDA developers but not novices.","tokens":1478,"name":"115.jsnp"}
{"score":"68","reasoning":"Code implements RBM contrastive divergence with CUDA kernels for hidden and visible units, handling large/small connections and random values. Logical flow is clear but readability suffers from abrupt start, undeclared variables, dense pointer arithmetic, magic numbers like MAX_THREADS_PER_BLOCK, and no comments. Suitable for experts but hard for newcomers.","tokens":1255,"name":"33.jsnp"}
{"score":"68","reasoning":"Repetitive structure for vectorized alignment computation is consistent and performance-oriented, with helpful comments explaining steps like saving old H and updating E F. However, cryptic variable names (regH0.w regT etc), untyped registers, and domain-specific macros (sub_sat tex2D) make it hard for non-experts to follow without full context. Unrolled loop is clear but verbose.","tokens":1628,"name":"113.jsnp"}
{"score":"65","reasoning":"Code shows standard CUDA patterns with kernel launches dim3 blocks grids and error checks but suffers from fragmentation incomplete functions cryptic long names like Block_reg_convertNMIGradientFromVoxelToRealSpace assuming external defines poor indentation and heavy reliance on undeclared constants making it moderately hard to comprehend without context.","tokens":1483,"name":"110.jsnp"}
{"score":"82","reasoning":"Straightforward C functions with simple nested loops and clear intent: write_data outputs first half-plus-one points per channel, trim_spectrum copies and normalizes them. Good use of ppc variable, proper indentation. Lacks comment on write_data, assumes even division, no file error checks, inclusive loop may copy one extra point, but logic is easy to follow for C programmers.","tokens":1243,"name":"119.jsnp"}
{"score":"25","reasoning":"Heavy reliance on macros like CUDA_GENERIC_MD4 and DUPLICATEDNTLM_CUDA_KERNEL_CREATE_LONG obscures logic. Extremely long argument lists with dozens of parameters p0-p47 repeated across lines hinder readability. Repetitive kernel instantiations for lengths 1-13 add clutter without comments or clear documentation. Comprehensible only with full macro context.","tokens":1424,"name":"16.jsnp"}
{"score":"68","reasoning":"Code structure is clean with descriptive function name and consistent CUDA_SAFE_CALL usage. However, top macros MD5_CUDA_KERNEL_CREATE_LONG are undefined and unclear, comment incorrectly states copying to host instead of device, threadId parameter is unused, and magic numbers like 8192 rely on external defines. Readable for CUDA experts but confusing for others.","tokens":1160,"name":"43.jsnp"}
{"score":"82","reasoning":"The code demonstrates good readability for CUDA experts with clear steps: symbol binding, texture binding, memset, grid calculation, kernel launch, sync, and debug print. Uses safe calls and descriptive names like voxelNumber, activeVoxelNumber. Standard practices enhance comprehension. Minor deductions for undeclared variables, external macros like Block_reg_getVoxelBasedNMIGradientUsingPW, and domain-specific terms assuming prior knowledge.","tokens":1528,"name":"25.jsnp"}
{"score":"82","reasoning":"Well-structured CUDA kernel for column convolution with shared memory tiling halos upper lower and main data loads unrolled loops for efficiency and syncthreads before compute. Host launcher has asserts for dimensions. Complex indexing and baseY offset with pitch handling are standard but tricky. Relies on undefined macros like COLUMNS_BLOCKDIM_X Y RESULT_STEPS HALO_STEPS and c_Kernel reducing self-containment. Readable for CUDA experts familiar with tiled convolutions.","tokens":1783,"name":"10.jsnp"}
{"score":"25","reasoning":"Incomplete code snippet with an extremely long parameter list full of complex pointer types and custom cudafloat, hindering readability. No comments or documentation. Shared memory declarations are straightforward but overall structure is hard to comprehend quickly due to density and abrupt end.","tokens":1044,"name":"6.jsnp"}
{"score":"82","reasoning":"Code shows good readability with meaningful variable names like start end mode clear initialization and standard C practices for command line parsing. Comments provide context. Indentation is consistent. Minor deductions for abrupt snippet start incomplete function and reliance on external functions like shrCheckCmdLineFlag which assume familiarity reducing standalone comprehension.","tokens":1116,"name":"74.jsnp"}
{"score":"98","reasoning":"The code presents two straightforward getter methods in C++ for a neural network class. They are concise, use const correctness appropriately, and include bounds checking with assert for safety. Variable and method names are descriptive, making the purpose clear: returning layer count and neuron count per layer. Highly readable and easy to comprehend with no unnecessary complexity.","tokens":906,"name":"91.jsnp"}
{"score":"68","reasoning":"Code snippet includes database cleanup loop, run method for loading data and printing info, and quicksort comparator. Logic is straightforward and uses clear variable names, but fragmented structure, inconsistent indentation, missing context for globals/members like params and dbSeqs, and abrupt ending reduce comprehension. Suitable for experts but not beginners.","tokens":1359,"name":"95.jsnp"}
{"score":"88","reasoning":"Standard CUDA kernel launch with synchronization and conditional verbose printf. Uses descriptive kernel name and common patterns like CUDA_SAFE_CALL. Printf is multi-line but readable. Assumes prior variable definitions, no comments, but straightforward for experts. High readability with minor issues.","tokens":1084,"name":"56.jsnp"}
{"score":"92","reasoning":"Well-structured CUDA kernel for 3D image smoothing along z-axis using textures. Clear tid calculation, bounds check, intuitive index and z offset for window, straightforward accumulation loop with in-bounds check. Variable names descriptive, logic easy to follow. Lacks comments and relies on external vars like windowSize, but highly comprehensible for CUDA users.","tokens":1478,"name":"9.jsnp"}
{"score":"62","reasoning":"Code defines constants and inline function clearly but relies on undefined custom types like float_2 float_3 mat_44 requiring prior knowledge. Matrix operations use manual indexing which is error-prone and verbose. No error handling for calloc or functions. Missing includes and abrupt endif reduce clarity. Comprehensible for domain experts but not beginners.","tokens":1336,"name":"94.jsnp"}
{"score":"62","reasoning":"The code snippet is incomplete, starting mid-function with unexplained closing brace. Good kernel comments explain intent, but relies on undefined macros like BETTER_THAN and IMUL, undeclared shared memory s_addends, and texture t_texFitnesses. Shared memory neighbor loading uses shifted indexing which is clever but confusing, with critical syncthreads commented out. Toroidal wrap-around logic adds complexity. Overall moderate readability for CUDA experts, harder for others.","tokens":2530,"name":"26.jsnp"}
{"score":"65","reasoning":"CUDA C host functions for PSO-like optimization. Strengths: clear naming, proper dim3 usage, section comments. Weaknesses: abrupt incomplete start, repetitive long if-else chain in h_findBestFitness with hardcoded block sizes and shared mem, no abstraction like loop or switch. getThreadNumForReduction uses float math awkwardly. Comprehensible for experts but verbose and unpolished.","tokens":1500,"name":"78.jsnp"}
{"score":"62","reasoning":"Structurally simple nested loops with clear indentation. Computation is straightforward: copies normalized half-range data per channel. However, cryptic variable names (ppc, h_h, h_o, norm) obscure purpose without context. No comments. Potential issue with integer division (ppc/2) if ppc odd, risking off-by-one in loop and indexing. Moderately comprehensible for experts, harder for others.","tokens":1262,"name":"80.jsnp"}
{"score":"32","reasoning":"Code heavily relies on undefined macros like NUM_NEURONS, NUM_OUTPUTS, NEURON, OUTPUT_NEURON, and PATTERN, making it incomprehensible without full context. Dense CUDA shared memory usage, pointer arithmetic, and indexing lack clarity. No comments or explanations. Incomplete snippet adds confusion. Suitable only for experts familiar with exact setup.","tokens":1535,"name":"87.jsnp"}
{"score":"35","reasoning":"Poor readability due to inconsistent indentation, excessive leading spaces on first line, and abrupt loop closure. Commented line includes hardcoded magic numbers like 128*128*50, reducing clarity. Logic of accumulating sum_activity with exponential attenuation and indexing is comprehensible but formatting hinders quick understanding.","tokens":925,"name":"2.jsnp"}
{"score":"78","reasoning":"Clean CUDA kernel loop using standard threadIdx.x and blockDim.x striding for parallel neuron processing. Intent is clear: load input, check Inf/NaN, load weights/bias. Descriptive vars like numNeurons, inputs. But incomplete (cuts off in else), relies on undefined cudafloat type, SAMPLE macro, IsInfOrNaN func, CUDA_VALUE, no comments, assumes CUDA expertise.","tokens":1043,"name":"52.jsnp"}
{"score":"25","reasoning":"Code suffers from extremely long lines with excessive parameter repetition (b0 to b15 listed 5+ times per call), backslash continuations, and heavy macro usage like clearB0toB15 and incrementCounters. Lacks comments, indentation is poor in snippet, and assumes external macro definitions. Dense CUDA-specific hashing logic hard for general readers to comprehend quickly.","tokens":2013,"name":"114.jsnp"}
{"score":"65","reasoning":"Concise CUDA kernel launcher using standard dim3 blocks and grid computation with ceil. Clear purpose for joint histogram on GPU. Issues: typo in d_jont_hist parameter, undefined BLOCK constant, pointer-to-pointer args slightly obscure dereferencing, commented unused memcpy line, missing includes for ceil. Readable for CUDA experts but confusing for beginners due to assumptions and errors.","tokens":1099,"name":"118.jsnp"}
{"score":"62","reasoning":"Repetitive if statements for powers 9-15 reduce readability and could be a loop. Cryptic macros like AS BS and variables tx ty aEnd lack context. Proper syncthreads usage and boundary checks are good. Indexing in output is understandable but uses magic BLOCK_SIZE. Kernel launch code is clear. Moderately comprehensible for CUDA experts.","tokens":1485,"name":"86.jsnp"}
{"score":"65","reasoning":"Code intent is clear from comments describing GPU parameter and data copying steps. Straightforward operations for CUDA setup. However, typos like subsitution, missing spaces in function args like gapOpen,gapExtend, prefixed unusual names like swMemcpyParameters and pMemcpyToArray, abrupt start with closing brace, and indentation issues hinder full readability and professionalism.","tokens":1251,"name":"49.jsnp"}
{"score":"88","reasoning":"Standard CUDA parallel reduction for min value and position using shared memory. Follows efficient unrolled pattern with size checks and syncthreads. Volatile usage correct for last warp. Repetitive structure is idiomatic for performance. Clear variable names. Snippet incompleteness and minor indentation quirks slightly hinder full comprehension.","tokens":1497,"name":"59.jsnp"}
{"score":"35","reasoning":"Code fragment uses bit smearing idiom for population count, efficient but highly cryptic without comments or context. Incomplete CUDA kernel with abrupt shared memory declaration adds confusion. Clean syntax but poor readability for non-experts due to lack of explanations and fragmentation.","tokens":1027,"name":"29.jsnp"}
{"score":"25","reasoning":"Code is dense with undefined macros like MD4HH and MD4Sxx, obscuring the MD4 hash computation steps. Backslash continuations create long unwieldy lines. Nested ifs on pass_len and long checkHash128LENTLM call with many params reduce clarity. Macro concatenations like ##pass_len assume prior knowledge. Repetitive MAKE_MFN calls are boilerplate. Comprehensible only in full CUDA cracking tool context for experts.","tokens":1752,"name":"69.jsnp"}
{"score":"25","reasoning":"Code is highly repetitive macro calls from 19 to 48 with identical structure and no comments or explanations. Pattern is obvious but lacks expressiveness, assumes macro knowledge, and would benefit from generation via loop. Low readability for maintenance or newcomers.","tokens":1117,"name":"83.jsnp"}
{"score":"42","reasoning":"Code snippet from MD5 implementation uses undefined macros like MD5GG and MD5HH requiring prior knowledge. Partial rounds with comments aid structure but commented lines and abrupt switch to incomplete MD5_Reverse function reduce clarity. Dense hex constants and variable rotations assume crypto expertise. Readable for specialists but poor for general audience.","tokens":2048,"name":"53.jsnp"}
{"score":"72","reasoning":"The code demonstrates basic CUDA memory allocation and memcpy operations clearly with linear flow. Loops are simple and purpose is evident. However, it lacks comments, includes, error checks for malloc, memory cleanup with free/cudaFree, and has repetitive code blocks. Casting to uint8_t is consistent but verbose. Minor indentation issues. Comprehensible for CUDA users but improvable for general readability.","tokens":1145,"name":"22.jsnp"}
{"score":"82","reasoning":"Well-structured C++ CUDA code for NMF multiplicative divergence update. Clear variable names for dimensions n m r. Concise logic with standard matrix multiplies and kernel launches. Descriptive kernel names like UpdateH_MD. Assumes library knowledge DeviceMatrix cudafloat and NMF expertise. Lacks inline comments reducing general comprehension but highly readable for domain experts.","tokens":1467,"name":"75.jsnp"}
{"score":"62","reasoning":"Code employs descriptive function names and familiar CUDA launch syntax, aiding partial readability. However, heavy reliance on undeclared variables like J I w b a, custom cudafloat type, magic numbers such as MAX_THREADS_PER_BLOCK INITIAL_BIAS_HIDDEN_UNITS, and conditional compilation with ifdef obscure intent. Lack of comments and incomplete snippet hinder full comprehension.","tokens":1614,"name":"30.jsnp"}
{"score":"78","reasoning":"Code is readable with clear comments, descriptive variable names, and proper indentation. It effectively uses CUDA texture fetches for position transformation and sampling. However, repetitive manual matrix multiplication for x y z components could be looped for better conciseness. Long lines and unusual bounds check (-1 to size) slightly reduce ease of comprehension.","tokens":1217,"name":"71.jsnp"}
{"score":"82","reasoning":"Code structure is clear with defined TRANSFER_SIZE, straightforward memory allocation, initialization loop, and CUDA operations. Variable names like hostPtr, devPtr are intuitive. Deductions for missing includes (stdio.h, stdlib.h, cuda_runtime.h), no error checks on cudaMemcpy, incomplete cleanup (free backPtr and cudaFree devPtr missing), and commented sections adding minor confusion. Still easy to follow for CUDA C users.","tokens":1383,"name":"4.jsnp"}
{"score":"62","reasoning":"CUDA kernel snippet for error and delta computation in reconstruction task. Uses standard idioms like threadIdx and syncthreads well. Logic is traceable: loads data, computes errors, accumulates deltas for weight updates. However, short cryptic names (dat, rec, vd), no comments, incomplete context, magic vars (I,J,samples), minor indentation issues hinder quick comprehension for non-experts.","tokens":1206,"name":"44.jsnp"}
{"score":"72","reasoning":"Code follows standard CUDA patterns for symbol copies, grid setup, and kernel launch with descriptive vars like imageSize and detectorOrigin. Proper use of dim3 and float3. However, undeclared BLOCK constant, external symbols like c_ImageSize, commented dead code, unclear dim indexing (grid_y on dim[3]), and no allocation check for d_positionField hinder standalone comprehension.","tokens":1610,"name":"79.jsnp"}
{"score":"35","reasoning":"Code snippet is fragmented and incomplete, lacking opening braces for structs and proper closure, making it hard to parse. Variable names like kernPtr and kernName are abbreviated and unclear. Comments provide some context but use C++ style // in C. Indentation is inconsistent, reducing overall comprehension.","tokens":1174,"name":"70.jsnp"}
{"score":"25","reasoning":"The code snippet is poorly formatted with inconsistent indentation and awkward line breaks, making it hard to read. The if condition checks for NaN using self-equality which is clever but unexplained, reducing comprehension without comments. Incomplete context and abrupt end further hinder understanding. Variable names are descriptive but overall readability is low.","tokens":878,"name":"77.jsnp"}
{"score":"65","reasoning":"Inconsistent indentation in cleanup code makes it hard to follow. run() function is well-structured with helpful comments and clear printf outputs. Undeclared variables like i and globals assumed. Incomplete comparator at end. Suitable for experienced C++ devs but needs formatting fixes for better readability.","tokens":1536,"name":"76.jsnp"}
{"score":"45","reasoning":"Snippet shows matrix updates in CUDA context with custom DeviceMatrix class. Readability hampered by short undefined variable names like W V deltaH aux H deltaH2, magic constants SIZE_BLOCKS_NMF, commented lines causing confusion, and kernel launches without explanations. Clear sequence and header comment help but assumes deep domain knowledge reducing general comprehension.","tokens":1258,"name":"32.jsnp"}
{"score":"65","reasoning":"Code shows clear intent in computing entropy derivatives for X Y Z using similar operations but suffers from high repetition with copy-pasted blocks for each dimension hurting readability. Variable names are descriptive and logic is straightforward. Sparse comments and nested loops add slight complexity but structure is consistent making it moderately easy to comprehend for familiar readers.","tokens":1253,"name":"117.jsnp"}
{"score":"62","reasoning":"CUDA kernel snippet for weight updates in neural network training. Uses standard primitives like syncthreads and threadIdx but starts abruptly without declarations or full context. Cryptic variable names like vd hd vr hr assume domain knowledge. Indentation inconsistent logic flow clear for experts but hard for general readers. Incomplete ending reduces comprehension.","tokens":1205,"name":"48.jsnp"}
{"score":"72","reasoning":"Code is concise with clear function name and standard CUDA NIfTI patterns for copying image sizes to GPU constants. Readable for experts familiar with the domain. Lacks comments purpose explanation assumes CUDA_SAFE_CALL macro and symbol knowledge. Incomplete but logic is straightforward and easy to follow.","tokens":982,"name":"1.jsnp"}
{"score":"98","reasoning":"The code snippet is very short, simple, and well-structured. It features a clear header comment, a commented include, and three straightforward macro definitions with descriptive names, proper alignment, and standard ternary logic for MAX. No complexity or errors, making it highly readable and easy to comprehend.","tokens":877,"name":"109.jsnp"}
{"score":"25","reasoning":"This CUDA kernel snippet uses heavy unrolling with 16 individual variables for blocks and positions (b0-b15, p0-p15), resulting in excessively long parameter lists spanning multiple lines with backslashes. Dense calls to undefined functions/macros like clearB0toB15 and CUDA_GENERIC_MD5 reduce clarity. Shared memory and host copy function add complexity without comments or structure, making it hard to follow without deep domain knowledge.","tokens":1884,"name":"108.jsnp"}
{"score":"45","reasoning":"This CUDA kernel snippet ends with synchronization and conditional matrix scaling using a macro WMATRIX. Readability is low due to missing context for variables like y x n r sum1 sum2 sumH and macro definition. Logic is simple but assumes prior block setup and unrolled loop via x +\u003d16. Suitable for CUDA experts only lacks comments and declarations.","tokens":1015,"name":"66.jsnp"}
{"score":"15","reasoning":"The code snippet is fragmented and incomplete, lacking full function definitions and declarations for variables like idnx idny sum centers_width input_width. Inconsistent indentation poor formatting and extra closing braces cause syntax issues. Mixes device kernel and host launcher code abruptly. Typos like cudafloat and KERNEL suggest errors. Overall very low readability and comprehension.","tokens":1150,"name":"15.jsnp"}
{"score":"42","reasoning":"Code snippet is fragmented with abrupt starts and ends, inconsistent indentation, commented-out lines, and lacks context or includes. Memory allocations lack immediate error checks, dummy structures use self-referential circular lists without clear purpose. Overall logic is understandable for C experts but hard for others due to missing definitions and abrupt style.","tokens":1209,"name":"37.jsnp"}
{"score":"32","reasoning":"Dense code with excessive line continuations using backslashes, making it hard to parse visually. Heavy reliance on undefined macros, functions like reduceSingleCharsetNormal and SHA_TRANSFORM, and cryptic constants like SHA1_Candidate_Device_Chain_Length. Repetitive assignments for a b c d e variables. No comments or whitespace for clarity, assumes deep domain knowledge of SHA1 and GPU optimization.","tokens":1822,"name":"84.jsnp"}
{"score":"25","reasoning":"Inconsistent indentation with mixed spaces and tabs makes structure unclear. Commented-out code including a conditional and TODO block adds confusion without explanation. Fragmented closing braces and obscure function call cpgebuf lack context, hindering comprehension of flow and purpose.","tokens":911,"name":"54.jsnp"}
{"score":"98","reasoning":"The snippet is minimal, containing only standard C include directives for libraries like stdio, stdlib, string, and CUDA utilities, plus a simple macro definition for MAX_STEPS. Layout is clean, names are descriptive, no logic or complexity present, ensuring maximum readability and comprehension ease. Minor deduction for unusual header name with leading underscore.","tokens":915,"name":"40.jsnp"}
{"score":"15","reasoning":"The snippet is a series of 9 repetitive macro invocations with a very long, all-caps name MD5SALTEDMD5SALTPASS_CUDA_KERNEL_CREATE and sequential arguments 8 to 16. No comments, context, or macro definition provided, rendering it cryptic and hard to comprehend. Repetition suggests generated code but lacks readability aids like loops or explanations.","tokens":1135,"name":"45.jsnp"}
{"score":"92","reasoning":"Code is concise with descriptive variable names like d_localGradient and dimNeuronsPatterns. Performs clear operations: resizing array, calculating shared memory sizes, setting CUDA dim3 dimensions. Highly readable for CUDA experts in neural network context. No redundancy or errors, though lacks comments.","tokens":1038,"name":"8.jsnp"}
{"score":"88","reasoning":"Well-structured C socket code with clear comments explaining actions, consistent indentation, descriptive variable names like sock_fd and client_fd, and standard error handling via perror and exit. Uses IPv6 properly with inet_ntop. Minor issues: incomplete net_recv function and closing listening sock_fd on accept error which may not be ideal, but readability remains high for experienced C programmers.","tokens":1318,"name":"17.jsnp"}
{"score":"70","reasoning":"The code correctly computes CUDA grid size using ceil, defines dim3 blocks and grid, launches the joint histogram kernel with proper parameters, and synchronizes. It follows standard CUDA idioms concisely. However, non-descriptive names like B and G, potential typo in d_jont_hist, unclear et_ prefix, missing includes or defines for BLOCK, and no comments hinder broader readability and comprehension.","tokens":1141,"name":"63.jsnp"}
{"score":"42","reasoning":"Code defines a macro-generated CUDA kernel for SHA1 hash chains. Intent is clear from structure and variable names, but readability is hindered by very long parameter lists in calls like LoadMD5RegistersFromGlobalMemory, undefined helper functions, numerous magic constants, unrolled block variables b0-b15, no comments, and dense bit operations. Assumes deep project context, making comprehension difficult for outsiders.","tokens":1964,"name":"38.jsnp"}
{"score":"68","reasoning":"Code shows clear intent for CUDA joint histogram computation using shared memory binning and single-threaded atomic-free updates. Good use of clamping and conditional write check. However, critical flaw: __syncthreads() inside if(tid \u003c array_size) risks deadlock if block has threads beyond array_size. Repeated A/B code, no comments, uninitialized shared mem for unused slots, and assumed vars like tid reduce ease of comprehension.","tokens":2579,"name":"27.jsnp"}
